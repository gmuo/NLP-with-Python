{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCpEJocTRc4z",
    "outputId": "ef43a1aa-d8ea-45eb-e3cd-abd1ac3cc620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hRu4NDe2SpMt"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WiOlWRUBS2zp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3BoEVXmS4TP",
    "outputId": "52267482-73d3-406a-80c6-217080e1f1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rkn0x3GSS8yg"
   },
   "outputs": [],
   "source": [
    "rutaData = \"/content/drive/MyDrive/3º - GCD/2C - LENGUAJE NATURAL Y RECUPERACIÓN DE LA INFORMACIÓN/Prácticas LNR/Práctica 3 (Sesiones 1 a 5) - LNR/\"\n",
    "rutaNumpys = \"/content/drive/MyDrive/3º - GCD/2C - LENGUAJE NATURAL Y RECUPERACIÓN DE LA INFORMACIÓN/Prácticas LNR/Práctica 3 (Sesiones 1 a 5) - LNR/02 - Segunda Entrega - Práctica 3/\"\n",
    "\n",
    "rutaDataBorja = \"/content/drive/MyDrive/Práctica 3 (Sesiones 1 a 5) - LNR/\"\n",
    "rutaNumpyBorja = \"/content/drive/MyDrive/Práctica 3 (Sesiones 1 a 5) - LNR/02 - Segunda Entrega - Práctica 3/\"\n",
    "\n",
    "# Representación con Word Embedding - Ponderada\n",
    "data_pond = np.loadtxt(rutaNumpyBorja + \"rep_pond_w_emb.txt\")\n",
    "# Representación con Word Embedding - Mediana\n",
    "data_med = np.loadtxt(rutaNumpyBorja + \"rep_median_w_emb.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2MhWtBq_TGMH"
   },
   "outputs": [],
   "source": [
    "df_detests = pd.read_csv(rutaDataBorja + \"train.csv\")\n",
    "extra_Data = pd.DataFrame(df_detests['stereotype']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4HEebyFCTH8w"
   },
   "outputs": [],
   "source": [
    "df_detests_test = pd.read_csv(rutaDataBorja + \"test.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "etp9nMEYTIZl"
   },
   "outputs": [],
   "source": [
    "tokenizer= BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "example_text = ['Usando BETO en clases de ciencia de datos de la universidad politécnica.',\n",
    "\"Los estudiantes de este grado son muy aplicados y estudiosos.\"]\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 20, \n",
    "                       truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ag6LTbrMTQmi"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaVLXkYHTW7f",
    "outputId": "214bc23d-88db-4648-9aed-2953c3f10516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] los estudiantes de este grado son muy aplicados y estudiosos. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = tokenizer.decode(bert_input.input_ids[1])\n",
    "print(example_text)\n",
    "# Cargar el modelo pre-entrenado\n",
    "model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "# Cambiar el model a modo evaluación\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PaD22ZQoTcpx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8Rk2-fiTgrp",
    "outputId": "bbbf4644-7147-4ba6-d689-b9703c5a232e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 768])\n",
      "torch.Size([2, 768])\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**bert_input)\n",
    "    # Los modelos Transformers siempre devuelven tuplas.\n",
    "    # Aquí, el primer elemento se corresponde a los vectores en la salida de la última capa\n",
    "    # de BETO\n",
    "    encoded_layers = outputs[0]\n",
    "    print(encoded_layers.size())\n",
    "    #Aquí se obtiene el embedding de los tokens CLS para cada texto de entrada\n",
    "    #Esta representación sirve como un embedding contextual de los textos.\n",
    "    cls_vector = encoded_layers[:,0,:]\n",
    "    print(cls_vector.size())\n",
    "    #Vector asociado al token CLS del primer texto en la entrada.\n",
    "    cls_vector = cls_vector.cpu().detach().numpy()[0]\n",
    "    print(len(cls_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rT3vS0zdUGLD"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, mode=\"train\"):\n",
    "        self.mode =mode\n",
    "        if mode !=\"train\":\n",
    "            self.labels= np.asarray([0]*len( df['sentence']))\n",
    "        else:\n",
    "            self.labels = [int(label) for label in df['stereotype']]\n",
    "        self.texts = [tokenizer(text,padding='max_length', max_length = 512, truncation\n",
    "        = True, return_tensors=\"pt\") for text in df['sentence']]\n",
    "    def classes(self):\n",
    "        return self.labels \n",
    "    def __len__(self):\n",
    "        return len(self.labels) \n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "# CLASE CLASIFICADOR\n",
    "class BETOClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3, model_name='dccuchile/bert-base-spanish-wwm-uncased'):\n",
    "        super(BETOClassifier, self).__init__()\n",
    "        self.beto = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #CAPA DE SALIDA\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        #ACTIVACIÓN DE LA CAPA DE SALIDA\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.beto(input_ids = input_id, attention_mask= mask,\n",
    "        return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "31k1rRuuUH_-"
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size=8):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = loss.cuda()\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = loss(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "            model.zero_grad()\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                output = model(input_id, mask)\n",
    "                batch_loss = loss(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        print(\n",
    "            f'Epochs:{epoch_num+1}|Train Loss:{total_loss_train/len(train_data):.3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "# VALIDATION LOOP\n",
    "def evaluate(model, test_data, batch_size=8, evaltype=True):\n",
    "    test = Dataset(test_data, mode=\"test\")\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    total_acc_test = 0\n",
    "    predict=[]\n",
    "    out = None\n",
    "    with torch.no_grad():\n",
    "        k=0\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            output = model(input_id, mask)\n",
    "            #CONCATENER LAS PREDICCIONES DEL MODELO\n",
    "            if k == 0:\n",
    "                out = output\n",
    "            else:\n",
    "                out = torch.cat((out, output), 0)\n",
    "            k+=1\n",
    "            #SI SE CONOCEN LAS ETIQUETAS DEL TEST SE PUEDE CALCULAR EL ACC, EN OTRO CASO\n",
    "            #SOLO SE DEBE RETORNAR LA SALIDA DEL MODELO\n",
    "            if evaltype:\n",
    "                acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "                total_acc_test += acc\n",
    "\n",
    "    if evaltype:\n",
    "        print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    return out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72-q-jW3WTVW"
   },
   "source": [
    "## OPTIMIZACIÓN DE HIPERPARÁMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mbu7Kx8WYNL"
   },
   "source": [
    "Vamos a realizar diversas pruebas cambiando los valores de epoch, batch y learning rate con el fin de optimizar estos hiperparámetros y obtener el mejor resultado posible. Como en las anteriores prácticas, nos guiaremos principalmente por el valor del f1 score aunque no despreciaremos el valor de otras métricas. Dado que con la función predeterminada solo podemos observar el accuracy obtenido en train y validación, lo que hemos hecho es realizar la predicción de los datos de validación, sacar el vector correspondiente a las predicciones y compararlos con las observaciones reales. Así, con estos dos vectores disponibles, mediante las funciones de la librería de scikit learn, podemos mostrar la matriz de confusión y algunas métricas más de nuestro interés como son el f1 score.\n",
    "\n",
    "Desconocemos si es posible realizar esto prescindiendo de scikit learn y realizandolo solo mediante BERT y es por eso que hemos hecho uso de esta biblioteca\n",
    "\n",
    "Mencionar que se intentó realizar el balanceo de clases mediante la técnica SMOTE pero nos vimos con muchas complicaciones a la hora de pasarle estos nuevos datos a BERT. Al no disponer de la documentación suficiente (pese a una búsqueda exahustiva por internet no encontramos nada decente) para informarnos de donde venía el fallo y ver si podíamos solucionarlo, decidimos seguir adelante con el conjunto sin balancear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEqcdGZDUg4S",
    "outputId": "de297d0d-7629-46f4-899f-1c0a6ebbe37c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_detests, test_size=0.2)\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 1\n",
    "BATCH = 8\n",
    "model = BETOClassifier()\n",
    "LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxb4P7hvUoal",
    "outputId": "6517bc4e-6384-4f31-c919-9524ccc406ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:11<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.086             | Train Accuracy:  0.552             | Val Loss:  0.086             | Val Accuracy:  0.572\n"
     ]
    }
   ],
   "source": [
    "bert_8cap = train(model, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cYSVoIGMWcEM"
   },
   "outputs": [],
   "source": [
    "pred_Beto = evaluate(model, df_val, BATCH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlAfRvobW6pG",
    "outputId": "10f5dd02-e9f1-4b11-e51a-c85fff4cf349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[369, 225],\n",
       "       [105,  65]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicc=pred_Beto.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df_val['stereotype'], predicc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex04QwJmZG_7",
    "outputId": "b0882724-586f-415e-be14-a41f45814a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69       594\n",
      "           1       0.22      0.38      0.28       170\n",
      "\n",
      "    accuracy                           0.57       764\n",
      "   macro avg       0.50      0.50      0.49       764\n",
      "weighted avg       0.66      0.57      0.60       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv-InYi9U5N1"
   },
   "source": [
    "### PRUEBA CON 9 BATCHS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2v342gSU-1c",
    "outputId": "7b631c69-3273-48d9-e243-b3b839a43d0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 340/340 [03:14<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.079             | Train Accuracy:  0.548             | Val Loss:  0.078             | Val Accuracy:  0.577\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 1\n",
    "BATCH = 9\n",
    "model9 = BETOClassifier()\n",
    "LR = 1e-6\n",
    "bert_9cap = train(model9, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pEDpiScSVhYs"
   },
   "outputs": [],
   "source": [
    "pred_Beto9 = evaluate(model9, df_val, BATCH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v82V7SqLVsh4",
    "outputId": "7f7ae223-894f-4172-cdb0-814e4baaf5fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[393, 201],\n",
       "       [103,  67]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicc9=pred_Beto9.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df_val['stereotype'], predicc9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8PBcyAeVyzn",
    "outputId": "be7dec6e-3226-43aa-d86b-d012e054bd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       594\n",
      "           1       0.25      0.39      0.31       170\n",
      "\n",
      "    accuracy                           0.60       764\n",
      "   macro avg       0.52      0.53      0.51       764\n",
      "weighted avg       0.67      0.60      0.63       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kigUFBEhQAq"
   },
   "source": [
    "### Prueba con 8 batchs y 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99o7iAkphTjX",
    "outputId": "9c787440-bcd1-4189-b002-6314e48a403e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 382/382 [03:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.088             | Train Accuracy:  0.515             | Val Loss:  0.087             | Val Accuracy:  0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:12<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2|Train Loss:0.087             | Train Accuracy:  0.527             | Val Loss:  0.087             | Val Accuracy:  0.558\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 2\n",
    "BATCH = 8\n",
    "model12 = BETOClassifier()\n",
    "LR = 1e-6\n",
    "bert_12cap = train(model12, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRn4GkvThVMA",
    "outputId": "7896f05c-cf9b-4093-ac80-015759da6401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[355 239]\n",
      " [ 89  81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.68       594\n",
      "           1       0.25      0.48      0.33       170\n",
      "\n",
      "    accuracy                           0.57       764\n",
      "   macro avg       0.53      0.54      0.51       764\n",
      "weighted avg       0.68      0.57      0.61       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Beto12 = evaluate(model12, df_val, BATCH, False)\n",
    "predicc12=pred_Beto12.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(df_val['stereotype'], predicc12))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe_sf-cCltxF"
   },
   "source": [
    "### 8 batchs y 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRswDrNalwET",
    "outputId": "5d3aa686-645d-4d17-9e96-048f3b1f2545"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 382/382 [03:11<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.085             | Train Accuracy:  0.628             | Val Loss:  0.085             | Val Accuracy:  0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:08<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2|Train Loss:0.085             | Train Accuracy:  0.609             | Val Loss:  0.085             | Val Accuracy:  0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:09<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:3|Train Loss:0.084             | Train Accuracy:  0.632             | Val Loss:  0.084             | Val Accuracy:  0.666\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 3\n",
    "BATCH = 8\n",
    "model12 = BETOClassifier()\n",
    "LR = 1e-6\n",
    "bert_12cap = train(model12, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgK_xWEclymc",
    "outputId": "bf44560e-c92a-44a2-93dc-46471f6145df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[457 137]\n",
      " [109  61]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       594\n",
      "           1       0.31      0.36      0.33       170\n",
      "\n",
      "    accuracy                           0.68       764\n",
      "   macro avg       0.56      0.56      0.56       764\n",
      "weighted avg       0.70      0.68      0.69       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Beto12 = evaluate(model12, df_val, BATCH, False)\n",
    "predicc12=pred_Beto12.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(df_val['stereotype'], predicc12))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbc-eRmOXhzo"
   },
   "source": [
    "A la vista de los resultados, vemos que los 3 modelos obtienen resultados bastante pobres. La potencia de cálculo de nuestros ordenadores nos impide aumentar el número de epochs y batch (cuando superamos 3 epochs y 8 batch, el sistema nos devuelve un error) por lo que el mejor modelo que podemos obtener en las condiciones en las que nos encontramos es el de 3 epochs y 8 batchs. Ahora variaremos el learning rate para ver si conseguimos unos mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZeQTjbCo2GH"
   },
   "source": [
    "## Variando el learning rate. \n",
    "\n",
    "### Learning rate = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3IAqoHlpAWz",
    "outputId": "def50976-d156-48ac-c7cb-1c227e968cc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 382/382 [03:09<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.087             | Train Accuracy:  0.519             | Val Loss:  0.088             | Val Accuracy:  0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:13<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2|Train Loss:0.087             | Train Accuracy:  0.520             | Val Loss:  0.088             | Val Accuracy:  0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:13<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:3|Train Loss:0.087             | Train Accuracy:  0.513             | Val Loss:  0.087             | Val Accuracy:  0.533\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 3\n",
    "BATCH = 8\n",
    "model12 = BETOClassifier()\n",
    "LR = 1e-8\n",
    "bert_12cap = train(model12, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mabhYdbYpG1P",
    "outputId": "da8a8139-ee49-49ea-de21-680df1625ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[285 309]\n",
      " [ 68 102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.48      0.60       594\n",
      "           1       0.25      0.60      0.35       170\n",
      "\n",
      "    accuracy                           0.51       764\n",
      "   macro avg       0.53      0.54      0.48       764\n",
      "weighted avg       0.68      0.51      0.55       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Beto12 = evaluate(model12, df_val, BATCH, False)\n",
    "predicc12=pred_Beto12.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(df_val['stereotype'], predicc12))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcVjzpwtsdUM"
   },
   "source": [
    "### Learning rate 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NM1FjO3jsg1I",
    "outputId": "b9d1a4fd-6747-47d3-e6ea-fb103c956488"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 382/382 [03:12<00:00,  1.99it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.081             | Train Accuracy:  0.713             | Val Loss:  0.080             | Val Accuracy:  0.738\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:13<00:00,  1.97it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2|Train Loss:0.081             | Train Accuracy:  0.722             | Val Loss:  0.080             | Val Accuracy:  0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:12<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:3|Train Loss:0.081             | Train Accuracy:  0.718             | Val Loss:  0.081             | Val Accuracy:  0.712\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 3\n",
    "BATCH = 8\n",
    "model12 = BETOClassifier()\n",
    "LR = 1e-10\n",
    "bert_12cap = train(model12, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NW5fuaifskBm",
    "outputId": "b28b122b-84ab-4034-a0ff-3669803c4c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[528  66]\n",
      " [151  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       594\n",
      "           1       0.22      0.11      0.15       170\n",
      "\n",
      "    accuracy                           0.72       764\n",
      "   macro avg       0.50      0.50      0.49       764\n",
      "weighted avg       0.65      0.72      0.68       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Beto12 = evaluate(model12, df_val, BATCH, False)\n",
    "predicc12=pred_Beto12.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(df_val['stereotype'], predicc12))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwP-XOTZxPtU"
   },
   "source": [
    "### Learning rate = 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILRglgXcxTwi",
    "outputId": "512078d4-bb87-4526-af42-62ac17aa719f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 382/382 [03:05<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss:0.078             | Train Accuracy:  0.761             | Val Loss:  0.078             | Val Accuracy:  0.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:07<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2|Train Loss:0.078             | Train Accuracy:  0.760             | Val Loss:  0.078             | Val Accuracy:  0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [03:07<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:3|Train Loss:0.078             | Train Accuracy:  0.760             | Val Loss:  0.078             | Val Accuracy:  0.772\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#HIPERPARAMETROS\n",
    "EPOCHS = 3\n",
    "BATCH = 8\n",
    "model12 = BETOClassifier()\n",
    "LR = 1e-12\n",
    "bert_12cap = train(model12, df_train, df_val, LR, EPOCHS, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f-RIsxFxX0p",
    "outputId": "3f87bf61-2ce9-4496-9e90-b2481daa1fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[588   6]\n",
      " [168   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       594\n",
      "           1       0.25      0.01      0.02       170\n",
      "\n",
      "    accuracy                           0.77       764\n",
      "   macro avg       0.51      0.50      0.45       764\n",
      "weighted avg       0.66      0.77      0.68       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Beto12 = evaluate(model12, df_val, BATCH, False)\n",
    "predicc12=pred_Beto12.tolist()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(df_val['stereotype'], predicc12))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_val['stereotype'], predicc12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGu58thf3xrV"
   },
   "source": [
    "Como vemos, conforme aumentamos el learning rate, lo que hace el modelo es ignorar la clase 1, prediciendo prácticamente todas las observaciones como clase 0. Es por eso que obtenemos un accuracy tan relativamente elevado. Pero, fijándono en la matriz de confusión, vemos que las métricas para la clase 1 son rídiculas, por lo que concluimos que este modelo es inutilizable. Una posible mejora sería balancear ambas clases pero como hemos comentado antes, se realizaron varias pruebas sin éxito para esta labor.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PruebasBert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
