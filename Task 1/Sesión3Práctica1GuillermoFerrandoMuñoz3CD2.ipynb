{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LNR - Práctica 1\n",
    "## Sesión 3: NLTK. POS tagging\n",
    "\n",
    "Nombre:\n",
    "- Guillermo Ferrando Muñoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Procesamiento del corpus cess_esp anotado con información morfosintáctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. Descargar corpus usando NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Procesar el corpus para transformar la anotación de las etiquetas\n",
    "originales (289 etiquetas) a un conjunto reducido (66 etiquetas).\n",
    "Para realizar esta transformación utilizar los siguientes criterios:\n",
    "todas las etiquetas serán de longitud igual a 2 por defecto, salvo\n",
    "los verbos (v) y los signos de puntuación (F) que pueden ser de\n",
    "tres. También pueden existir etiquetas de longitud =1. En el\n",
    "conjunto transformado también se deben eliminar anotaciones\n",
    "de la forma: `(u'*0*', u'sn')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BracketParseCorpusReader in module nltk.corpus.reader.bracket_parse object:\n",
      "\n",
      "class BracketParseCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      " |  BracketParseCorpusReader(root, fileids, comment_char=None, detect_blocks='unindented_paren', encoding='utf8', tagset=None)\n",
      " |  \n",
      " |  Reader for corpora that consist of parenthesis-delineated parse trees,\n",
      " |  like those found in the \"combined\" section of the Penn Treebank,\n",
      " |  e.g. \"(S (NP (DT the) (JJ little) (NN dog)) (VP (VBD barked)))\".\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BracketParseCorpusReader\n",
      " |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root, fileids, comment_char=None, detect_blocks='unindented_paren', encoding='utf8', tagset=None)\n",
      " |      :param root: The root directory for this corpus.\n",
      " |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      " |      :param comment_char: The character which can appear at the start of\n",
      " |          a line to indicate that the rest of the line is a comment.\n",
      " |      :param detect_blocks: The method that is used to find blocks\n",
      " |        in the corpus; can be 'unindented_paren' (every unindented\n",
      " |        parenthesis starts a new parse) or 'sexpr' (brackets are\n",
      " |        matched).\n",
      " |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      " |            for normalizing or converting the POS tags returned by the\n",
      " |            tagged_...() methods.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      " |  \n",
      " |  parsed_sents(self, fileids=None)\n",
      " |  \n",
      " |  raw(self, fileids=None)\n",
      " |  \n",
      " |  sents(self, fileids=None)\n",
      " |  \n",
      " |  tagged_sents(self, fileids=None, tagset=None)\n",
      " |  \n",
      " |  tagged_words(self, fileids=None, tagset=None)\n",
      " |  \n",
      " |  words(self, fileids=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  citation(self)\n",
      " |      Return the contents of the corpus citation.bib file, if it exists.\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  fileids(self)\n",
      " |      Return a list of file identifiers for the fileids that make up\n",
      " |      this corpus.\n",
      " |  \n",
      " |  license(self)\n",
      " |      Return the contents of the corpus LICENSE file, if it exists.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  readme(self)\n",
      " |      Return the contents of the corpus README file, if it exists.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cess_esp.ensure_loaded()\n",
    "help(cess_esp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el método `tagged_sents()` es el que contiene las etiquetas originales. Vamos a comprobar si tiene 289 como se indica en el enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "originales = cess_esp.tagged_sents()\n",
    "lista=[]\n",
    "for i in originales:\n",
    "    for j,k in i:\n",
    "        if k not in lista:\n",
    "            lista.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista) #comprobado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a transformar el corpus en un conjunto reducido con 66 etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformadas=[]\n",
    "conjunto=set() #para contar las no repetidas\n",
    "for i in originales:\n",
    "    listaparcial=[]\n",
    "    for j,k in i:\n",
    "        if j != \"*0*\":\n",
    "            if 'v'==k[0] or 'F'==k[0]:\n",
    "                listaparcial.append((j,k[0:3]))\n",
    "                conjunto.add(k[0:3])\n",
    "            else:\n",
    "                listaparcial.append((j,k[0:2]))\n",
    "                conjunto.add(k[0:2])\n",
    "    transformadas.append(listaparcial)\n",
    "len(conjunto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "print(originales[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da'), ('grupo', 'nc'), ('estatal', 'aq'), ('Electricité_de_France', 'np'), ('-Fpa-', 'Fpa'), ('EDF', 'np'), ('-Fpt-', 'Fpt'), ('anunció', 'vmi'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da'), ('compra', 'nc'), ('del', 'sp'), ('51_por_ciento', 'Zp'), ('de', 'sp'), ('la', 'da'), ('empresa', 'nc'), ('mexicana', 'aq'), ('Electricidad_Águila_de_Altamira', 'np'), ('-Fpa-', 'Fpa'), ('EAA', 'np'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq'), ('por', 'sp'), ('el', 'da'), ('japonés', 'aq'), ('Mitsubishi_Corporation', 'np'), ('para', 'sp'), ('poner_en_marcha', 'vmn'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('de', 'sp'), ('495', 'Z'), ('megavatios', 'nc'), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "print(transformadas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Uso de etiquetadores morfosintácticos (usar hmm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module nltk.tag.hmm in nltk.tag:\n",
      "\n",
      "NAME\n",
      "    nltk.tag.hmm\n",
      "\n",
      "DESCRIPTION\n",
      "    Hidden Markov Models (HMMs) largely used to assign the correct label sequence\n",
      "    to sequential data or assess the probability of a given label and data\n",
      "    sequence. These models are finite state machines characterised by a number of\n",
      "    states, transitions between these states, and output symbols emitted while in\n",
      "    each state. The HMM is an extension to the Markov chain, where each state\n",
      "    corresponds deterministically to a given event. In the HMM the observation is\n",
      "    a probabilistic function of the state. HMMs share the Markov chain's\n",
      "    assumption, being that the probability of transition from one state to another\n",
      "    only depends on the current state - i.e. the series of states that led to the\n",
      "    current state are not used. They are also time invariant.\n",
      "    \n",
      "    The HMM is a directed graph, with probability weighted edges (representing the\n",
      "    probability of a transition between the source and sink states) where each\n",
      "    vertex emits an output symbol when entered. The symbol (or observation) is\n",
      "    non-deterministically generated. For this reason, knowing that a sequence of\n",
      "    output observations was generated by a given HMM does not mean that the\n",
      "    corresponding sequence of states (and what the current state is) is known.\n",
      "    This is the 'hidden' in the hidden markov model.\n",
      "    \n",
      "    Formally, a HMM can be characterised by:\n",
      "    \n",
      "    - the output observation alphabet. This is the set of symbols which may be\n",
      "      observed as output of the system.\n",
      "    - the set of states.\n",
      "    - the transition probabilities *a_{ij} = P(s_t = j | s_{t-1} = i)*. These\n",
      "      represent the probability of transition to each state from a given state.\n",
      "    - the output probability matrix *b_i(k) = P(X_t = o_k | s_t = i)*. These\n",
      "      represent the probability of observing each symbol in a given state.\n",
      "    - the initial state distribution. This gives the probability of starting\n",
      "      in each state.\n",
      "    \n",
      "    To ground this discussion, take a common NLP application, part-of-speech (POS)\n",
      "    tagging. An HMM is desirable for this task as the highest probability tag\n",
      "    sequence can be calculated for a given sequence of word forms. This differs\n",
      "    from other tagging techniques which often tag each word individually, seeking\n",
      "    to optimise each individual tagging greedily without regard to the optimal\n",
      "    combination of tags for a larger unit, such as a sentence. The HMM does this\n",
      "    with the Viterbi algorithm, which efficiently computes the optimal path\n",
      "    through the graph given the sequence of words forms.\n",
      "    \n",
      "    In POS tagging the states usually have a 1:1 correspondence with the tag\n",
      "    alphabet - i.e. each state represents a single tag. The output observation\n",
      "    alphabet is the set of word forms (the lexicon), and the remaining three\n",
      "    parameters are derived by a training regime. With this information the\n",
      "    probability of a given sentence can be easily derived, by simply summing the\n",
      "    probability of each distinct path through the model. Similarly, the highest\n",
      "    probability tagging sequence can be derived with the Viterbi algorithm,\n",
      "    yielding a state sequence which can be mapped into a tag sequence.\n",
      "    \n",
      "    This discussion assumes that the HMM has been trained. This is probably the\n",
      "    most difficult task with the model, and requires either MLE estimates of the\n",
      "    parameters or unsupervised learning using the Baum-Welch algorithm, a variant\n",
      "    of EM.\n",
      "    \n",
      "    For more information, please consult the source code for this module,\n",
      "    which includes extensive demonstration code.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        HiddenMarkovModelTrainer\n",
      "    nltk.tag.api.TaggerI(builtins.object)\n",
      "        HiddenMarkovModelTagger\n",
      "    \n",
      "    class HiddenMarkovModelTagger(nltk.tag.api.TaggerI)\n",
      "     |  HiddenMarkovModelTagger(symbols, states, transitions, outputs, priors, transform=<function _identity at 0x0000015AFCD2DD90>)\n",
      "     |  \n",
      "     |  Hidden Markov model class, a generative model for labelling sequence data.\n",
      "     |  These models define the joint probability of a sequence of symbols and\n",
      "     |  their labels (state transitions) as the product of the starting state\n",
      "     |  probability, the probability of each state transition, and the probability\n",
      "     |  of each observation being generated from each state. This is described in\n",
      "     |  more detail in the module documentation.\n",
      "     |  \n",
      "     |  This implementation is based on the HMM description in Chapter 8, Huang,\n",
      "     |  Acero and Hon, Spoken Language Processing and includes an extension for\n",
      "     |  training shallow HMM parsers or specialized HMMs as in Molina et.\n",
      "     |  al, 2002.  A specialized HMM modifies training data by applying a\n",
      "     |  specialization function to create a new training set that is more\n",
      "     |  appropriate for sequential tagging with an HMM.  A typical use case is\n",
      "     |  chunking.\n",
      "     |  \n",
      "     |  :param symbols: the set of output symbols (alphabet)\n",
      "     |  :type symbols: seq of any\n",
      "     |  :param states: a set of states representing state space\n",
      "     |  :type states: seq of any\n",
      "     |  :param transitions: transition probabilities; Pr(s_i | s_j) is the\n",
      "     |      probability of transition from state i given the model is in\n",
      "     |      state_j\n",
      "     |  :type transitions: ConditionalProbDistI\n",
      "     |  :param outputs: output probabilities; Pr(o_k | s_i) is the probability\n",
      "     |      of emitting symbol k when entering state i\n",
      "     |  :type outputs: ConditionalProbDistI\n",
      "     |  :param priors: initial state distribution; Pr(s_i) is the probability\n",
      "     |      of starting in state i\n",
      "     |  :type priors: ProbDistI\n",
      "     |  :param transform: an optional function for transforming training\n",
      "     |      instances, defaults to the identity function.\n",
      "     |  :type transform: callable\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HiddenMarkovModelTagger\n",
      "     |      nltk.tag.api.TaggerI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, symbols, states, transitions, outputs, priors, transform=<function _identity at 0x0000015AFCD2DD90>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  best_path(self, unlabeled_sequence)\n",
      "     |      Returns the state sequence of the optimal (most probable) path through\n",
      "     |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      "     |      programming.\n",
      "     |      \n",
      "     |      :return: the state sequence\n",
      "     |      :rtype: sequence of any\n",
      "     |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      "     |      :type unlabeled_sequence: list\n",
      "     |  \n",
      "     |  best_path_simple(self, unlabeled_sequence)\n",
      "     |      Returns the state sequence of the optimal (most probable) path through\n",
      "     |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      "     |      programming.  This uses a simple, direct method, and is included for\n",
      "     |      teaching purposes.\n",
      "     |      \n",
      "     |      :return: the state sequence\n",
      "     |      :rtype: sequence of any\n",
      "     |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      "     |      :type unlabeled_sequence: list\n",
      "     |  \n",
      "     |  entropy(self, unlabeled_sequence)\n",
      "     |      Returns the entropy over labellings of the given sequence. This is\n",
      "     |      given by::\n",
      "     |      \n",
      "     |          H(O) = - sum_S Pr(S | O) log Pr(S | O)\n",
      "     |      \n",
      "     |      where the summation ranges over all state sequences, S. Let\n",
      "     |      *Z = Pr(O) = sum_S Pr(S, O)}* where the summation ranges over all state\n",
      "     |      sequences and O is the observation sequence. As such the entropy can\n",
      "     |      be re-expressed as::\n",
      "     |      \n",
      "     |          H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]\n",
      "     |          = log Z - sum_S Pr(S | O) log Pr(S, 0)\n",
      "     |          = log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]\n",
      "     |      \n",
      "     |      The order of summation for the log terms can be flipped, allowing\n",
      "     |      dynamic programming to be used to calculate the entropy. Specifically,\n",
      "     |      we use the forward and backward probabilities (alpha, beta) giving::\n",
      "     |      \n",
      "     |          H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)\n",
      "     |          + sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)\n",
      "     |          + sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)\n",
      "     |      \n",
      "     |      This simply uses alpha and beta to find the probabilities of partial\n",
      "     |      sequences, constrained to include the given state(s) at some point in\n",
      "     |      time.\n",
      "     |  \n",
      "     |  log_probability(self, sequence)\n",
      "     |      Returns the log-probability of the given symbol sequence. If the\n",
      "     |      sequence is labelled, then returns the joint log-probability of the\n",
      "     |      symbol, state sequence. Otherwise, uses the forward algorithm to find\n",
      "     |      the log-probability over all label sequences.\n",
      "     |      \n",
      "     |      :return: the log-probability of the sequence\n",
      "     |      :rtype: float\n",
      "     |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      "     |          property, and optionally the TAG property\n",
      "     |      :type sequence:  Token\n",
      "     |  \n",
      "     |  point_entropy(self, unlabeled_sequence)\n",
      "     |      Returns the pointwise entropy over the possible states at each\n",
      "     |      position in the chain, given the observation sequence.\n",
      "     |  \n",
      "     |  probability(self, sequence)\n",
      "     |      Returns the probability of the given symbol sequence. If the sequence\n",
      "     |      is labelled, then returns the joint probability of the symbol, state\n",
      "     |      sequence. Otherwise, uses the forward algorithm to find the\n",
      "     |      probability over all label sequences.\n",
      "     |      \n",
      "     |      :return: the probability of the sequence\n",
      "     |      :rtype: float\n",
      "     |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      "     |          property, and optionally the TAG property\n",
      "     |      :type sequence:  Token\n",
      "     |  \n",
      "     |  random_sample(self, rng, length)\n",
      "     |      Randomly sample the HMM to generate a sentence of a given length. This\n",
      "     |      samples the prior distribution then the observation distribution and\n",
      "     |      transition distribution for each subsequent observation and state.\n",
      "     |      This will mostly generate unintelligible garbage, but can provide some\n",
      "     |      amusement.\n",
      "     |      \n",
      "     |      :return:        the randomly created state/observation sequence,\n",
      "     |                      generated according to the HMM's probability\n",
      "     |                      distributions. The SUBTOKENS have TEXT and TAG\n",
      "     |                      properties containing the observation and state\n",
      "     |                      respectively.\n",
      "     |      :rtype:         list\n",
      "     |      :param rng:     random number generator\n",
      "     |      :type rng:      Random (or any object with a random() method)\n",
      "     |      :param length:  desired output length\n",
      "     |      :type length:   int\n",
      "     |  \n",
      "     |  reset_cache(self)\n",
      "     |  \n",
      "     |  tag(self, unlabeled_sequence)\n",
      "     |      Tags the sequence with the highest probability state sequence. This\n",
      "     |      uses the best_path method to find the Viterbi path.\n",
      "     |      \n",
      "     |      :return: a labelled sequence of symbols\n",
      "     |      :rtype: list\n",
      "     |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      "     |      :type unlabeled_sequence: list\n",
      "     |  \n",
      "     |  test(self, test_sequence, verbose=False, **kwargs)\n",
      "     |      Tests the HiddenMarkovModelTagger instance.\n",
      "     |      \n",
      "     |      :param test_sequence: a sequence of labeled test instances\n",
      "     |      :type test_sequence: list(list)\n",
      "     |      :param verbose: boolean flag indicating whether training should be\n",
      "     |          verbose or include printed output\n",
      "     |      :type verbose: bool\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(labeled_sequence, test_sequence=None, unlabeled_sequence=None, **kwargs) from abc.ABCMeta\n",
      "     |      Train a new HiddenMarkovModelTagger using the given labeled and\n",
      "     |      unlabeled training instances. Testing will be performed if test\n",
      "     |      instances are provided.\n",
      "     |      \n",
      "     |      :return: a hidden markov model tagger\n",
      "     |      :rtype: HiddenMarkovModelTagger\n",
      "     |      :param labeled_sequence: a sequence of labeled training instances,\n",
      "     |          i.e. a list of sentences represented as tuples\n",
      "     |      :type labeled_sequence: list(list)\n",
      "     |      :param test_sequence: a sequence of labeled test instances\n",
      "     |      :type test_sequence: list(list)\n",
      "     |      :param unlabeled_sequence: a sequence of unlabeled training instances,\n",
      "     |          i.e. a list of sentences represented as words\n",
      "     |      :type unlabeled_sequence: list(list)\n",
      "     |      :param transform: an optional function for transforming training\n",
      "     |          instances, defaults to the identity function, see ``transform()``\n",
      "     |      :type transform: function\n",
      "     |      :param estimator: an optional function or class that maps a\n",
      "     |          condition's frequency distribution to its probability\n",
      "     |          distribution, defaults to a Lidstone distribution with gamma = 0.1\n",
      "     |      :type estimator: class or function\n",
      "     |      :param verbose: boolean flag indicating whether training should be\n",
      "     |          verbose or include printed output\n",
      "     |      :type verbose: bool\n",
      "     |      :param max_iterations: number of Baum-Welch interations to perform\n",
      "     |      :type max_iterations: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.tag.api.TaggerI:\n",
      "     |  \n",
      "     |  evaluate(self, gold)\n",
      "     |      Score the accuracy of the tagger against the gold standard.\n",
      "     |      Strip the tags from the gold standard text, retag it using\n",
      "     |      the tagger, then compute the accuracy score.\n",
      "     |      \n",
      "     |      :type gold: list(list(tuple(str, str)))\n",
      "     |      :param gold: The list of tagged sentences to score the tagger on.\n",
      "     |      :rtype: float\n",
      "     |  \n",
      "     |  tag_sents(self, sentences)\n",
      "     |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      "     |      \n",
      "     |          return [self.tag(sent) for sent in sentences]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HiddenMarkovModelTrainer(builtins.object)\n",
      "     |  HiddenMarkovModelTrainer(states=None, symbols=None)\n",
      "     |  \n",
      "     |  Algorithms for learning HMM parameters from training data. These include\n",
      "     |  both supervised learning (MLE) and unsupervised learning (Baum-Welch).\n",
      "     |  \n",
      "     |  Creates an HMM trainer to induce an HMM with the given states and\n",
      "     |  output symbol alphabet. A supervised and unsupervised training\n",
      "     |  method may be used. If either of the states or symbols are not given,\n",
      "     |  these may be derived from supervised training.\n",
      "     |  \n",
      "     |  :param states:  the set of state labels\n",
      "     |  :type states:   sequence of any\n",
      "     |  :param symbols: the set of observation symbols\n",
      "     |  :type symbols:  sequence of any\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, states=None, symbols=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  train(self, labeled_sequences=None, unlabeled_sequences=None, **kwargs)\n",
      "     |      Trains the HMM using both (or either of) supervised and unsupervised\n",
      "     |      techniques.\n",
      "     |      \n",
      "     |      :return: the trained model\n",
      "     |      :rtype: HiddenMarkovModelTagger\n",
      "     |      :param labelled_sequences: the supervised training data, a set of\n",
      "     |          labelled sequences of observations\n",
      "     |          ex: [ (word_1, tag_1),...,(word_n,tag_n) ]\n",
      "     |      :type labelled_sequences: list\n",
      "     |      :param unlabeled_sequences: the unsupervised training data, a set of\n",
      "     |          sequences of observations\n",
      "     |          ex: [ word_1, ..., word_n ]\n",
      "     |      :type unlabeled_sequences: list\n",
      "     |      :param kwargs: additional arguments to pass to the training methods\n",
      "     |  \n",
      "     |  train_supervised(self, labelled_sequences, estimator=None)\n",
      "     |      Supervised training maximising the joint probability of the symbol and\n",
      "     |      state sequences. This is done via collecting frequencies of\n",
      "     |      transitions between states, symbol observations while within each\n",
      "     |      state and which states start a sentence. These frequency distributions\n",
      "     |      are then normalised into probability estimates, which can be\n",
      "     |      smoothed if desired.\n",
      "     |      \n",
      "     |      :return: the trained model\n",
      "     |      :rtype: HiddenMarkovModelTagger\n",
      "     |      :param labelled_sequences: the training data, a set of\n",
      "     |          labelled sequences of observations\n",
      "     |      :type labelled_sequences: list\n",
      "     |      :param estimator: a function taking\n",
      "     |          a FreqDist and a number of bins and returning a CProbDistI;\n",
      "     |          otherwise a MLE estimate is used\n",
      "     |  \n",
      "     |  train_unsupervised(self, unlabeled_sequences, update_outputs=True, **kwargs)\n",
      "     |      Trains the HMM using the Baum-Welch algorithm to maximise the\n",
      "     |      probability of the data sequence. This is a variant of the EM\n",
      "     |      algorithm, and is unsupervised in that it doesn't need the state\n",
      "     |      sequences for the symbols. The code is based on 'A Tutorial on Hidden\n",
      "     |      Markov Models and Selected Applications in Speech Recognition',\n",
      "     |      Lawrence Rabiner, IEEE, 1989.\n",
      "     |      \n",
      "     |      :return: the trained model\n",
      "     |      :rtype: HiddenMarkovModelTagger\n",
      "     |      :param unlabeled_sequences: the training data, a set of\n",
      "     |          sequences of observations\n",
      "     |      :type unlabeled_sequences: list\n",
      "     |      \n",
      "     |      kwargs may include following parameters:\n",
      "     |      \n",
      "     |      :param model: a HiddenMarkovModelTagger instance used to begin\n",
      "     |          the Baum-Welch algorithm\n",
      "     |      :param max_iterations: the maximum number of EM iterations\n",
      "     |      :param convergence_logprob: the maximum change in log probability to\n",
      "     |          allow convergence\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "    \n",
      "    demo_bw()\n",
      "    \n",
      "    demo_pos()\n",
      "    \n",
      "    demo_pos_bw(test=10, supervised=20, unsupervised=10, verbose=True, max_iterations=5)\n",
      "    \n",
      "    load_pos(num_sents)\n",
      "    \n",
      "    logsumexp2(arr)\n",
      "\n",
      "DATA\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\guill\\anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import hmm \n",
    "help(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Dividir el corpus en dos partes: training (el 90% de las primeras\n",
    "frases) y de test (el 10% restante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=int(len(transformadas)*0.9) #índice de partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=transformadas[:tr]\n",
    "test=transformadas[tr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Saber entrenar el etiquetador con la partición de entrenamiento\n",
    "previamente transformada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultando la ayuda de hmm, vemos que tenemos una clase llamada `vHiddenMarkovModelTagger`, con un método `train()`. Deduzco que ese es el método que necesito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetador=nltk.HiddenMarkovModelTagger.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Saber etiquetar un conjunto de test con el modelo aprendido.\n",
    "\n",
    "d. Evaluar las prestaciones de un etiquetador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy over 13434 tokens: 87.84\n"
     ]
    }
   ],
   "source": [
    "etiquetador.test(test) #el modelo tiene una precisión del 87,84%. Es un buen resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8784"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(etiquetador.evaluate(test),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejericicio 3: Hacer una evaluación de las prestaciones de etiquetado usando todo elcorpus (10-fold cross validation).\n",
    "Se propone hacer las 10 particiones usando el corpus reducido en el orden original y barajándolo (sugerencia: se puede usar el método shuffle importándolo del módulo random “from random import shuffle”). Comprobar si al barajar el corpus se observan diferencias en los resultados de cada partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method shuffle in module random:\n",
      "\n",
      "shuffle(x, random=None) method of random.Random instance\n",
      "    Shuffle list x in place, and return None.\n",
      "    \n",
      "    Optional argument random is a 0-argument function returning a\n",
      "    random float in [0.0, 1.0); if it is the default None, the\n",
      "    standard random.random will be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "help(shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'da'), ('grupo', 'nc'), ('estatal', 'aq'), ('Electricité_de_France', 'np'), ('-Fpa-', 'Fpa'), ('EDF', 'np'), ('-Fpt-', 'Fpt'), ('anunció', 'vmi'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da'), ('compra', 'nc'), ('del', 'sp'), ('51_por_ciento', 'Zp'), ('de', 'sp'), ('la', 'da'), ('empresa', 'nc'), ('mexicana', 'aq'), ('Electricidad_Águila_de_Altamira', 'np'), ('-Fpa-', 'Fpa'), ('EAA', 'np'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq'), ('por', 'sp'), ('el', 'da'), ('japonés', 'aq'), ('Mitsubishi_Corporation', 'np'), ('para', 'sp'), ('poner_en_marcha', 'vmn'), ('una', 'di'), ('central', 'nc'), ('de', 'sp'), ('gas', 'nc'), ('de', 'sp'), ('495', 'Z'), ('megavatios', 'nc'), ('.', 'Fp')]\n",
      "\n",
      "[('Las', 'da'), ('tropas', 'nc'), ('agresoras', 'aq'), ('encontrarían', 'vmi'), ('un', 'di'), ('país', 'nc'), ('indefenso', 'aq'), ('y', 'cc'), ('una', 'di'), ('población', 'nc'), ('destruida', 'aq'), (',', 'Fc'), ('muerta', 'aq'), ('o', 'cc'), ('incapacitada', 'aq'), (',', 'Fc'), ('pero', 'cc'), ('con', 'sp'), ('sus', 'dp'), ('ciudades', 'nc'), ('y', 'cc'), ('recursos', 'nc'), ('industriales', 'aq'), ('intactos', 'aq'), ('.', 'Fp')]\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "copiatransformadas = transformadas.copy() #hacemos una copia del conjunto transformado\n",
    "shuffle(copiatransformadas)\n",
    "print(transformadas[0])\n",
    "print()\n",
    "print(copiatransformadas[0]) #la primera línea de la copia no coincide, se ha barajado\n",
    "print()\n",
    "print(len(copiatransformadas)==len(transformadas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la librería sklearn para hacer los 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9284,\n",
       " 0.9787,\n",
       " 0.9786,\n",
       " 0.9792,\n",
       " 0.9797,\n",
       " 0.9616,\n",
       " 0.9622,\n",
       " 0.9708,\n",
       " 0.9628,\n",
       " 0.8784]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf=KFold(n_splits=10)\n",
    "kf.split(copiatransformadas)\n",
    "resultadosprev=[]\n",
    "for n,(tra,tes) in enumerate(kf.split(transformadas)):\n",
    "    #print(f\"TRAIN {n}: \", tra)\n",
    "    #print(f\"TEST {n}: \", tes)\n",
    "    trainn=transformadas[tra[0]:tra[len(tra)-1]]\n",
    "    testt=transformadas[tes[0]:tes[len(tes)-1]]\n",
    "    etiquetadornew = nltk.HiddenMarkovModelTagger.train(trainn)\n",
    "    precision = round(etiquetadornew.evaluate(testt),4)\n",
    "    resultadosprev.append(precision)\n",
    "resultadosprev #precisiones con los datos sin barajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9258,\n",
       " 0.9712,\n",
       " 0.9719,\n",
       " 0.9732,\n",
       " 0.9722,\n",
       " 0.9731,\n",
       " 0.9732,\n",
       " 0.9756,\n",
       " 0.9734,\n",
       " 0.9251]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf=KFold(n_splits=10)\n",
    "kf.split(copiatransformadas)\n",
    "resultados=[]\n",
    "for n,(tra,tes) in enumerate(kf.split(copiatransformadas)):\n",
    "    #print(f\"TRAIN {n}: \", tra)\n",
    "    #print(f\"TEST {n}: \", tes)\n",
    "    trainn=copiatransformadas[tra[0]:tra[len(tra)-1]]\n",
    "    testt=copiatransformadas[tes[0]:tes[len(tes)-1]]\n",
    "    etiquetadornew = nltk.HiddenMarkovModelTagger.train(trainn)\n",
    "    precision = round(etiquetadornew.evaluate(testt),4)\n",
    "    resultados.append(precision)\n",
    "resultados #precisiones con los datos barajados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, haremos unos gráficos de barras para comparar los resultados de los datos barajados y sin barajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "kf = 10\n",
    "ejex = np.arange(k)\n",
    "ancho = 0.4\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.ylim(0.86,0.99)\n",
    "plt.bar(ejex, resultadosprev, width=ancho, label=\"Sin Barajar\")\n",
    "plt.bar(ejex+ancho, resultados, width=ancho, label=\"Barajadas\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(ejex+ancho, ('1','2','3','4','5','6','7','8','9','10'))\n",
    "plt.title(\"Precisiones (accuracy) con 10-Fold\")\n",
    "plt.xlabel(\"Folds\")\n",
    "plt.ylabel(\"Precisiones (accuracy)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que los resultados son muy similares en sobre el total. En la primera mitad ganan los datos sin barajar y en el segunda los barajados. Eso sí, en el fold número 10, hay una notable diferencia con ventaja a los barajados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
